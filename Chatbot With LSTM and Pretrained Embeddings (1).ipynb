{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchdata torchsummary --upgrade typing-extensions torchtext torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007"
   },
   "outputs": [],
   "source": [
    "# load brown embeddings\n",
    "\n",
    "# from nltk.corpus import brown\n",
    "\n",
    "# nltk.download('brown')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# model = gensim.models.Word2Vec(brown.sents())\n",
    "# model.save('brown.embedding')\n",
    "\n",
    "# w2v = gensim.models.Word2Vec.load('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 78840 questions and 78840 answers in the training dataset.\n",
      "There are 8759 questions and 8759 answers in the validation dataset.\n",
      "There are 10570 questions and 10570 answers in the test dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8759    What important neopragmatist was Harthorne's s...\n",
       "8760    How was Whitehead's theory of gravitation rece...\n",
       "8761    What physicists in the field of quantum theory...\n",
       "8762    What affect  did the discovery of gravitationa...\n",
       "8763                        What are gravitational waves?\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and explore SQuAD1 data\n",
    "\n",
    "from torchtext.datasets import SQuAD1\n",
    "\n",
    "train, test = SQuAD1()   \n",
    "\n",
    "def LoadSQuAD(data):\n",
    "    df = {\"question\": [], \"answer\": []}\n",
    "    index = 0\n",
    "    for context, question, answers, indices in data:\n",
    "        if answers[0]:\n",
    "            df[\"question\"].append(question)\n",
    "            df[\"answer\"].append(answers[0])\n",
    "        index += 1\n",
    "    df_complete = pd.DataFrame.from_dict(df)\n",
    "    SRC = df_complete[\"question\"]\n",
    "    TRG = df_complete[\"answer\"]\n",
    "    return df_complete, SRC, TRG\n",
    "    \n",
    "SRC_and_TRG_train_complete, SRC_train_complete, TRG_train_complete = LoadSQuAD(train)\n",
    "len_val_data = SRC_train_complete.shape[0]//10\n",
    "\n",
    "SRC_train = SRC_train_complete.iloc[len_val_data:]\n",
    "SRC_val = SRC_train_complete.iloc[:len_val_data]\n",
    "TRG_train = TRG_train_complete.iloc[len_val_data:]\n",
    "TRG_val = TRG_train_complete.iloc[:len_val_data]\n",
    "\n",
    "_, SRC_test, TRG_test = LoadSQuAD(test)\n",
    "\n",
    "print('There are {} questions and {} answers in the training dataset.'.format(SRC_train.shape[0], TRG_train.shape[0]))\n",
    "print('There are {} questions and {} answers in the validation dataset.'.format(SRC_val.shape[0], TRG_val.shape[0]))\n",
    "print('There are {} questions and {} answers in the test dataset.'.format(SRC_test.shape[0], TRG_test.shape[0]))\n",
    "SRC_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a vocabulary class\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.index = {}\n",
    "        self.count = 0\n",
    "        self.words = {}\n",
    "    \n",
    "    # tokenize each sentence\n",
    "    def prepareText(self, text):\n",
    "        tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        return tokens\n",
    "    \n",
    "    # create a list of all words contained in the text\n",
    "    def indexWord(self, word):\n",
    "        if word not in self.words:\n",
    "            self.words[word] = self.count\n",
    "            self.index[str(self.count)] = word\n",
    "            self.count += 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # take in a sentence and returns a list of integers \n",
    "    def indexSentences(self, sentence):\n",
    "        tokens = self.prepareText(sentence)\n",
    "        return [self.words[token] for token in tokens]\n",
    "    \n",
    "    # fill a vocabulary object with contents\n",
    "    def fillVocab(self, series, print_every=1000):\n",
    "        self.indexWord('<pad>')\n",
    "        \n",
    "        count = 0\n",
    "        for sentence in series:\n",
    "            text = self.prepareText(sentence)\n",
    "            for t in text:\n",
    "                if(self.indexWord(t)):\n",
    "                    if count % print_every == 0:\n",
    "                        print('Adding word {} to our vocabulary.'.format(count))\n",
    "                    count += 1\n",
    "        print('Added {} words to vocabulary.'.format(len(self.words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding word 0 to our vocabulary.\n",
      "Adding word 10000 to our vocabulary.\n",
      "Adding word 20000 to our vocabulary.\n",
      "Adding word 30000 to our vocabulary.\n",
      "Adding word 40000 to our vocabulary.\n",
      "Adding word 50000 to our vocabulary.\n",
      "Adding word 60000 to our vocabulary.\n",
      "Added 64259 words to vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# instantiate a vocabulary object and fill it \n",
    "vocab = Vocab(name='SQuAD1_vocab')\n",
    "SRC_and_TRG_complete = pd.concat([SRC_train, TRG_train, SRC_val, TRG_val, SRC_test, TRG_test])\n",
    "vocab.fillVocab(SRC_and_TRG_complete, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('<pad>', 0), ('What', 1), ('important', 2), ('neopragmatist', 3), ('was', 4), ('Harthorne', 5), ('s', 6), ('student', 7), ('How', 8), ('Whitehead', 9), ('theory', 10), ('of', 11), ('gravitation', 12), ('received', 13), ('physicists', 14), ('in', 15), ('the', 16), ('field', 17), ('quantum', 18), ('have', 19), ('been', 20), ('influenced', 21), ('by', 22), ('affect', 23), ('did', 24), ('discovery', 25), ('gravitational', 26), ('waves', 27), ('on', 28), ('are', 29)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out first 30 items of the vocabulary\n",
    "dict(list(vocab.words.items())[:30]).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index and pad sentences to length of the longest sentence in the data set\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import LongTensor\n",
    "\n",
    "SRC_train_indices = [vocab.indexSentences(s) for s in SRC_train]\n",
    "TRG_train_indices = [vocab.indexSentences(s) for s in TRG_train]\n",
    "SRC_val_indices = [vocab.indexSentences(s) for s in SRC_val]\n",
    "TRG_val_indices = [vocab.indexSentences(s) for s in TRG_val]\n",
    "SRC_test_indices = [vocab.indexSentences(s) for s in SRC_test]\n",
    "TRG_test_indices = [vocab.indexSentences(s) for s in TRG_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences to max_length\n",
    "def padSequences(sequences, max_len):\n",
    "    padded_sequences = []\n",
    "    for s in sequences:\n",
    "        \n",
    "        # calculate the number of padding tokens needed\n",
    "        num_padding = max_len - len(s)\n",
    "        \n",
    "        # create a new sequence with padding tokens added to the end\n",
    "        padded_sequence = s + [vocab.words['<pad>']] * num_padding\n",
    "        \n",
    "        # convert the sequence to a LongTensor and add it to the list\n",
    "        padded_sequences.append(LongTensor(padded_sequence))\n",
    "    return padded_sequences\n",
    "\n",
    "# determine the maximum length of sentences\n",
    "max_len = max(max(len(s) for s in SRC_train_indices), \n",
    "              max(len(s) for s in TRG_train_indices), \n",
    "              max(len(s) for s in SRC_val_indices), \n",
    "              max(len(s) for s in TRG_val_indices),\n",
    "              max(len(s) for s in SRC_test_indices),\n",
    "              max(len(s) for s in TRG_test_indices))\n",
    "\n",
    "SRC_train_pad = torch.stack(padSequences(SRC_train_indices, max_len))\n",
    "TRG_train_pad = torch.stack(padSequences(TRG_train_indices, max_len))\n",
    "SRC_val_pad = torch.stack(padSequences(SRC_val_indices, max_len))\n",
    "TRG_val_pad = torch.stack(padSequences(TRG_val_indices, max_len))\n",
    "SRC_test_pad = torch.stack(padSequences(SRC_test_indices, max_len))\n",
    "TRG_test_pad = torch.stack(padSequences(TRG_test_indices, max_len))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "batch_size = 64  \n",
    "\n",
    "train_data = TensorDataset(SRC_train_pad, TRG_train_pad)\n",
    "val_data = TensorDataset(SRC_val_pad, TRG_val_pad)\n",
    "test_data = TensorDataset(SRC_test_pad, TRG_test_pad)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_loader:\n",
    "#     print(type(i))\n",
    "#     print(len(i))\n",
    "#     print(i)\n",
    "#     for j in i:\n",
    "#         print(type(j))\n",
    "#         print(j.dim())\n",
    "#         print(len(j))\n",
    "#         display(j)\n",
    "#         for k in j:\n",
    "#             print(type(k))\n",
    "#             print(k.dim())\n",
    "#             print(len(k))\n",
    "#             print(k)\n",
    "#             break\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm cell needs arguments input, (hidden state, cell state)'\n",
    "where for batched data input is (sequence lengt, batch size, input size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Encoder, Decoder and Seq2Seq modules\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_size, drop_prob=0.5):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # nn.Embedding provides a vector representation of the input\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        \n",
    "        # nn.LSTM expects the arguments [input, (hidden state, cell state)]\n",
    "        # for batched data input is expected to be (sequence lengt, batch size, input size)\n",
    "        # batch_first=True changes the order to (batch size, sequence length, input size)\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self, i):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the src vector\n",
    "        Outputs: o, the encoder outputs\n",
    "                h, the hidden state\n",
    "                c, the cell state\n",
    "        '''\n",
    "        \n",
    "        print('ENCODER')\n",
    "        print('encoder input before embedding:', i.size())\n",
    "        \n",
    "        embedded = self.embedding(i)\n",
    "        \n",
    "        print('embedding dimension:', embedded.dim())\n",
    "        print('embedding size:', embedded.size())\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "        o, (h, c) = self.lstm(embedded)\n",
    "        \n",
    "        print('lstm output size:', o.size())\n",
    "        print('lstm hidden size:', h.size())\n",
    "        print('lstm cell state size:', c.size())\n",
    "        print(' ')\n",
    "        \n",
    "        return h, c\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, output_size, embedding_size, hidden_size):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size        \n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "         \n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, i, h, c):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the target vector\n",
    "        Outputs: o, the prediction\n",
    "                h, the hidden state\n",
    "        '''\n",
    "        \n",
    "        print('DECODER')\n",
    "        print('initial input size before unsqueezing:', i.size())\n",
    "        \n",
    "        i = i.unsqueeze(0)\n",
    "        \n",
    "        print('decoder input before embedding (i.unsqueeze(0)):', i.size())\n",
    "        \n",
    "        embedded = self.embedding(i)\n",
    "\n",
    "        print('embedding dimension:', embedded.dim())\n",
    "        print('embedding size:', embedded.size())\n",
    "        \n",
    "        o, (h, c) = self.lstm(embedded, (h, c))\n",
    "        \n",
    "        print('lstm output size:', o.size())\n",
    "\n",
    "        o = self.output(o.squeeze(0))\n",
    "        \n",
    "        print('decoder output size:', o.size())\n",
    "        print(' ')\n",
    "        \n",
    "        return o, h, c       \n",
    "                \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_size, output_size, device=device):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size, embedding_size, drop_prob=drop_prob)\n",
    "        self.decoder = Decoder(output_size, embedding_size, hidden_size)\n",
    "        \n",
    "        assert self.encoder.hidden_size == self.decoder.hidden_size, \\\n",
    "            'hidden dimensions of encoder and decoder must be equal.'\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):      \n",
    "        \n",
    "        # create empty output tensor with shape (length of trg, batch size, trg vocab size)\n",
    "        # that will later be filled with the predictions of the decoder\n",
    "        outputs = torch.zeros(trg.shape[0], trg.shape[1], self.decoder.output_size).to(device)\n",
    "\n",
    "        # use last hidden state of encoder as initial state for decoder\n",
    "        decoder_hidden, decoder_cell = self.encoder(src)\n",
    "        \n",
    "        decoder_input = trg[0, :]\n",
    "    \n",
    "        print('SEQ2SEQ OUTPUT')\n",
    "        print('size of initialized outputs variable:', outputs.size())\n",
    "        print('size of one element in outputs:', outputs[0].size())\n",
    "        print(' ')\n",
    "        \n",
    "        # loop through elements in batch\n",
    "        for t in range(1, trg.shape[0]):\n",
    "            decoder_output, decoder_hidden, decoder_cell = self.decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "            outputs[t] = decoder_output\n",
    "            teacher_force = torch.rand(1) < teacher_forcing_ratio\n",
    "            # use token with highest score as output\n",
    "            top1 = decoder_output.argmax(1)\n",
    "            decoder_input = trg[t] if teacher_force else top1\n",
    "                 \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, train_loader, criterion, optimizer, device=device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for src, trg in train_loader:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        # reshape output and target to calculate loss\n",
    "        # (slice off the first column and flatten output to 2 dim)\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device=device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg in val_loader:\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output = model(src, trg, teacher_forcing_ratio=0.0)\n",
    "\n",
    "            # reshape output and target to calculate loss\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "input_size = len(vocab.words)\n",
    "output_size = len(vocab.words)\n",
    "### im tutorial auf https://www.kaggle.com/code/columbine/seq2seq-pytorch\n",
    "### INPUT_DIM = len(SRC.vocab)\n",
    "### OUTPUT_DIM = len(TRG.vocab)\n",
    "### ggf prüfen\n",
    "\n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "drop_prob = 0.5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model, optimizer and loss function\n",
    "model = Seq2Seq(input_size, hidden_size, hidden_size, output_size)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.words['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER\n",
      "encoder input before embedding: torch.Size([64, 43])\n",
      "embedding dimension: 3\n",
      "embedding size: torch.Size([64, 43, 512])\n",
      "lstm output size: torch.Size([64, 43, 512])\n",
      "lstm hidden size: torch.Size([1, 64, 512])\n",
      "lstm cell state size: torch.Size([1, 64, 512])\n",
      " \n",
      "SEQ2SEQ OUTPUT\n",
      "size of initialized outputs variable: torch.Size([64, 43, 64259])\n",
      "size of one element in outputs: torch.Size([43, 64259])\n",
      " \n",
      "DECODER\n",
      "initial input size before unsqueezing: torch.Size([43])\n",
      "decoder input before embedding (i.unsqueeze(0)): torch.Size([1, 43])\n",
      "embedding dimension: 3\n",
      "embedding size: torch.Size([1, 43, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 1, 512), got [1, 64, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-abd3f7496400>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-0bc07f7b39a5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# reshape output and target to calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-6ec0facabf7a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# loop through elements in batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mteacher_force\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-6ec0facabf7a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, i, h, c)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'embedding size:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lstm output size:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[0;32m--> 699\u001b[0;31m                                'Expected hidden[0] size {}, got {}')\n\u001b[0m\u001b[1;32m    700\u001b[0m         self.check_hidden_size(hidden[1], self.get_expected_cell_size(input, batch_sizes),\n\u001b[1;32m    701\u001b[0m                                'Expected hidden[1] size {}, got {}')\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    229\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 1, 512), got [1, 64, 512]"
     ]
    }
   ],
   "source": [
    "# initialize the minimum validation loss\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "# training\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device=device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device=device)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}')\n",
    "    print(f'Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\"')\n",
    "    print(f'Val Loss: {val_loss:.3f} | Val PPL: {math.exp(val_loss):7.3f}\"')\n",
    "    \n",
    "    # save the model if the validation loss is at a minimum value\n",
    "    if valid_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPL stands for \"perplexity\". According to https://www.educative.io/answers/what-is-perplexity-in-nlp,\n",
    "\n",
    "*Perplexity is a standard that evaluates how well a probability model can predict a sample. When applied to language models like GPT, it represents the exponentiated average negative log-likelihood of a sequence. In essence, a lower perplexity score suggests that the model has a higher certainty in its predictions.*\n",
    "\n",
    "See https://towardsdatascience.com/perplexity-in-language-models-87a196019a94 for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "best_model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_loader, criterion)\n",
    " \n",
    "print(f\"Test Loss : {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}\")\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful tutorial:\n",
    "https://www.kaggle.com/code/columbine/seq2seq-pytorch"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
