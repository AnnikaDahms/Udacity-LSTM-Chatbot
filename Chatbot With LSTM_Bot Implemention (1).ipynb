{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchdata\n",
      "  Downloading torchdata-0.5.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.6 MB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Collecting torchtext\n",
      "  Downloading torchtext-0.14.1-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 62.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.14.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 64.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: urllib3>=1.25 in /opt/conda/lib/python3.7/site-packages (from torchdata) (1.25.7)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from torchdata) (2.23.0)\n",
      "Collecting torch==1.13.1\n",
      "  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 887.5 MB 6.3 kB/s  eta 0:00:01     |█████▏                          | 142.0 MB 45.2 MB/s eta 0:00:17     |██████████████████▊             | 518.6 MB 80.0 MB/s eta 0:00:05     |█████████████████████▊          | 602.3 MB 87.3 MB/s eta 0:00:04\n",
      "\u001b[?25hCollecting portalocker>=2.0.0\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext) (4.43.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext) (1.21.2)\n",
      "Requirement already satisfied, skipping upgrade: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision) (7.0.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata) (3.0.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\"\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 67.8 MB/s eta 0:00:01    |██████████▊                     | 7.0 MB 67.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\"\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 317.1 MB 22 kB/s s eta 0:00:01██████████                  | 139.7 MB 95.6 MB/s eta 0:00:02          | 150.0 MB 95.6 MB/s eta 0:00:029 MB 76.9 MB/s eta 0:00:02MB/s eta 0:00:01██████████████████▎   | 280.8 MB 77.1 MB/s eta 0:00:01 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\"\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 557.1 MB 7.3 kB/s eta 0:00:01�██▋                           | 80.4 MB 3.0 MB/s eta 0:02:42                    | 85.2 MB 3.0 MB/s eta 0:02:4036               | 167.8 MB 2.6 MB/s eta 0:02:30    |██████████▏                     | 177.7 MB 2.6 MB/s eta 0:02:26    |███████████                     | 192.3 MB 2.6 MB/s eta 0:02:21      | 197.3 MB 2.6 MB/s eta 0:02:19.9 MB/s eta 0:01:572.9 MB/s eta 0:01:52��████████▊                 | 257.2 MB 1.7 MB/s eta 0:02:58████                 | 262.2 MB 1.7 MB/s eta 0:02:55�█████████████▋                | 271.7 MB 1.7 MB/s eta 0:02:49  | 289.5 MB 2.8 MB/s eta 0:01:37[K     |██████████████████▌             | 322.8 MB 2.8 MB/s eta 0:01:25[K     |██████████████████▉             | 327.7 MB 2.8 MB/s eta 0:01:23   | 336.6 MB 2.9 MB/s eta 0:01:18   | 341.5 MB 2.9 MB/s eta 0:01:16�████████▏           | 350.4 MB 2.9 MB/s eta 0:01:13��████████████████▍           | 355.6 MB 2.9 MB/s eta 0:01:11��████████████████████▎          | 369.7 MB 2.9 MB/s eta 0:01:06█████████████████▊          | 378.1 MB 2.9 MB/s eta 0:01:03�▊        | 412.6 MB 2.7 MB/s eta 0:00:54██████        | 416.8 MB 2.7 MB/s eta 0:00:52��████████████████▍       | 424.2 MB 2.7 MB/s eta 0:00:49  | 456.6 MB 2.7 MB/s eta 0:00:38 | 464.2 MB 2.7 MB/s eta 0:00:36�█████████     | 468.8 MB 2.7 MB/s eta 0:00:34��████████▍    | 477.2 MB 3.0 MB/s eta 0:00:28��███████████████████▏   | 490.7 MB 3.0 MB/s eta 0:00:23��███████████████████▋   | 498.9 MB 3.0 MB/s eta 0:00:20    |█████████████████████████████▍  | 511.6 MB 3.0 MB/s eta 0:00:166 MB 2.5 MB/s eta 0:00:137 MB 2.5 MB/s eta 0:00:09███████▎| 544.3 MB 2.5 MB/s eta 0:00:06\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[K     |████████████████████████████████| 849 kB 56.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\"->torch==1.13.1->torchdata) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\"->torch==1.13.1->torchdata) (45.2.0.post20200209)\n",
      "Installing collected packages: nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, typing-extensions, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, torch, portalocker, torchdata, torchtext, torchvision\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 portalocker-2.7.0 torch-1.13.1 torchdata-0.5.1 torchtext-0.14.1 torchvision-0.14.1 typing-extensions-4.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchdata --upgrade typing-extensions torchtext torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, time, math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Exploration of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 87599 questions and 87599 answers in the training dataset.\n",
      "There are 10570 questions and 10570 answers in the test dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    To whom did the Virgin Mary allegedly appear i...\n",
       "1    What is in front of the Notre Dame Main Building?\n",
       "2    The Basilica of the Sacred heart at Notre Dame...\n",
       "3                    What is the Grotto at Notre Dame?\n",
       "4    What sits on top of the Main Building at Notre...\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load SQuAD1 data\n",
    "\n",
    "from torchtext.datasets import SQuAD1\n",
    "train, test = SQuAD1()   \n",
    "\n",
    "def LoadSQuAD(data):\n",
    "    df = {\"question\": [], \"answer\": []}\n",
    "    index = 0\n",
    "    for context, question, answers, indices in data:\n",
    "        if answers[0]:\n",
    "            df[\"question\"].append(question)\n",
    "            df[\"answer\"].append(answers[0])\n",
    "        index += 1\n",
    "    df_complete = pd.DataFrame.from_dict(df)\n",
    "    SRC = df_complete[\"question\"]\n",
    "    TRG = df_complete[\"answer\"]\n",
    "    return SRC, TRG\n",
    "    \n",
    "SRC_train, TRG_train = LoadSQuAD(train)\n",
    "SRC_test, TRG_test = LoadSQuAD(test)\n",
    "\n",
    "# explore dataset\n",
    "print('There are {} questions and {} answers in the training dataset.'.format(SRC_train.shape[0], TRG_train.shape[0]))\n",
    "print('There are {} questions and {} answers in the test dataset.'.format(SRC_test.shape[0], TRG_test.shape[0]))\n",
    "SRC_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 10000 questions and 10000 answers in the training dataset.\n"
     ]
    }
   ],
   "source": [
    "# reduce size of training set to make training faster\n",
    "SRC_train = SRC_train.iloc[:10000]\n",
    "TRG_train = TRG_train.iloc[:10000]\n",
    "print('There are now {} questions and {} answers in the training dataset.'.format(SRC_train.shape[0], TRG_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a vocabulary class\n",
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.index = {}\n",
    "        self.count = 0\n",
    "        self.words = {}\n",
    "    \n",
    "    # tokenize each sentence\n",
    "    def prepareText(self, text):\n",
    "        tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        return tokens\n",
    "    \n",
    "    # create a list of all words contained in the text\n",
    "    def indexWord(self, word):\n",
    "        if word not in self.words:\n",
    "            self.words[word] = self.count\n",
    "            self.index[str(self.count)] = word\n",
    "            self.count += 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # takes in a sentence and returns a list of integers \n",
    "    def indexSentences(self, sentence):\n",
    "        tokens = self.prepareText(sentence)\n",
    "        return [self.words[token] for token in tokens]\n",
    "    \n",
    "    # takes in a sequence of integers and returns a list of words\n",
    "    def IndexToWord(self, sentence):\n",
    "        return [self.index[str(word)] for word in sentence]\n",
    "    \n",
    "    # fill a vocabulary object with contents\n",
    "    def fillVocab(self, series, print_every=1000):\n",
    "        \n",
    "        # add 'pad' as word to vocabulary to account for padding  \n",
    "        self.indexWord('<pad>')\n",
    "        \n",
    "        count = 0\n",
    "        for sentence in series:\n",
    "            text = self.prepareText(sentence)\n",
    "            for t in text:\n",
    "                if(self.indexWord(t)):\n",
    "                    if count % print_every == 0:\n",
    "                        print('Adding word {} to our vocabulary.'.format(count))\n",
    "                    count += 1\n",
    "        print('Added {} words to vocabulary.'.format(len(self.words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding word 0 to our vocabulary.\n",
      "Adding word 10000 to our vocabulary.\n",
      "Adding word 20000 to our vocabulary.\n",
      "Added 24112 words to vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# instantiate and fill a vocabulary object\n",
    "vocab = Vocab(name='SQuAD1_vocab')\n",
    "SRC_and_TRG_complete = pd.concat([SRC_train, TRG_train, SRC_test, TRG_test])\n",
    "vocab.fillVocab(SRC_and_TRG_complete, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('<pad>', 0), ('To', 1), ('whom', 2), ('did', 3), ('the', 4), ('Virgin', 5), ('Mary', 6), ('allegedly', 7), ('appear', 8), ('in', 9), ('1858', 10), ('Lourdes', 11), ('France', 12), ('What', 13), ('is', 14), ('front', 15), ('of', 16), ('Notre', 17), ('Dame', 18), ('Main', 19), ('Building', 20), ('The', 21), ('Basilica', 22), ('Sacred', 23), ('heart', 24), ('at', 25), ('beside', 26), ('to', 27), ('which', 28), ('structure', 29)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out first 30 items of the vocabulary\n",
    "dict(list(vocab.words.items())[:30]).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn words into indices\n",
    "SRC_train_indices = [vocab.indexSentences(s) for s in SRC_train]\n",
    "TRG_train_indices = [vocab.indexSentences(s) for s in TRG_train]\n",
    "SRC_test_indices = [vocab.indexSentences(s) for s in SRC_test]\n",
    "TRG_test_indices = [vocab.indexSentences(s) for s in TRG_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a sequence of integers and pads it to max_length\n",
    "def padSequences(sequences, max_len):\n",
    "    padded_sequences = []\n",
    "    for s in sequences:\n",
    "        \n",
    "        # calculate the number of padding tokens needed\n",
    "        num_padding = max_len - len(s)\n",
    "        \n",
    "        # create a new sequence with padding tokens added to the end\n",
    "        padded_sequence = s + [vocab.words['<pad>']] * num_padding\n",
    "        \n",
    "        # convert the sequence to a LongTensor and add it to the list\n",
    "        padded_sequences.append(torch.LongTensor(padded_sequence))\n",
    "    return padded_sequences\n",
    "\n",
    "# determine the maximum length of sentences in the dataset\n",
    "max_len = max(max(len(s) for s in SRC_train_indices), \n",
    "              max(len(s) for s in TRG_train_indices), \n",
    "              max(len(s) for s in SRC_test_indices),\n",
    "              max(len(s) for s in TRG_test_indices))\n",
    "\n",
    "# pad sequences to max_length\n",
    "SRC_train_pad = torch.stack(padSequences(SRC_train_indices, max_len))\n",
    "TRG_train_pad = torch.stack(padSequences(TRG_train_indices, max_len))\n",
    "SRC_test_pad = torch.stack(padSequences(SRC_test_indices, max_len))\n",
    "TRG_test_pad = torch.stack(padSequences(TRG_test_indices, max_len))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "batch_size = 32  \n",
    "\n",
    "train_data = TensorDataset(SRC_train_pad, TRG_train_pad)\n",
    "test_data = TensorDataset(SRC_test_pad, TRG_test_pad)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete stored variables that are not needed anymore\n",
    "# to free up memory space\n",
    "del(SRC_and_TRG_complete, \n",
    "    SRC_test, SRC_test_indices, SRC_test_pad, \n",
    "    SRC_train, SRC_train_indices, SRC_train_pad, \n",
    "    TRG_test, TRG_test_indices, TRG_test_pad, \n",
    "    TRG_train, TRG_train_indices, TRG_train_pad,\n",
    "    train, test, train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder, Decoder and Seq2Seq modules\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_size, drop_prob):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # nn.Embedding provides a vector representation of the input\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        \n",
    "        # nn.LSTM takes the arguments [input, (hidden state, cell state)] \n",
    "        # where for batched data input is expected to be (sequence lengt, batch size, input size).\n",
    "        # batch_first=True changes the order to (batch size, sequence length, input size), but\n",
    "        # with swapped batch and sequence dimensions, it's crucial to ensure that all batch-related \n",
    "        # indexes are handled on index 0 while sequence-related indexes are processed on index 1\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self, i):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the src vector\n",
    "        Outputs: h, the hidden state\n",
    "                c, the cell state\n",
    "        '''\n",
    "        embedded = self.embedding(i)\n",
    "        embedded = self.dropout(embedded)\n",
    "        o, (h, c) = self.lstm(embedded)\n",
    "        \n",
    "        return h, c\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, output_size, embedding_size, hidden_size):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size        \n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, i, h, c):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the target vector\n",
    "        Outputs: o, the prediction\n",
    "                h, the hidden state\n",
    "        '''       \n",
    "        i = i.unsqueeze(1)\n",
    "        embedded = self.embedding(i)\n",
    "        o, (h, c) = self.lstm(embedded, (h, c))\n",
    "        o = self.output(o.squeeze(0))\n",
    "        \n",
    "        return o, h, c       \n",
    "                \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_size, output_size, drop_prob, device=device, ):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size, embedding_size, drop_prob)\n",
    "        self.decoder = Decoder(output_size, embedding_size, hidden_size)\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):      \n",
    "        \n",
    "        # create empty output tensor with shape (batch size, length of trg, trg vocab size)\n",
    "        # that will later be filled with the predictions of the decoder\n",
    "        outputs = torch.zeros(trg.shape[0], trg.shape[1], self.decoder.output_size).to(device)\n",
    "\n",
    "        # use last hidden state of encoder as initial state for decoder\n",
    "        decoder_hidden, decoder_cell = self.encoder(src)\n",
    "        \n",
    "        decoder_input = trg[:, 0]\n",
    "\n",
    "        # loop through elements in batch\n",
    "        for t in range(1, trg.shape[1]):\n",
    "            decoder_output, decoder_hidden, decoder_cell = self.decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "            outputs[:, t, :] = decoder_output.view(*outputs[:, t, :].shape)\n",
    "            teacher_force = torch.rand(1) < teacher_forcing_ratio\n",
    "            # use token with highest score as output\n",
    "            top1 = decoder_output.argmax(2)\n",
    "            decoder_input = trg[:, t] if teacher_force else top1.squeeze(1)\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, train_loader, criterion, optimizer, device=device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for src, trg in tqdm(train_loader):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        # reshape output and target to calculate loss\n",
    "        # (slice off the first column and flatten output to 2 dim)\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "input_size = len(vocab.words)\n",
    "output_size = len(vocab.words)\n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "drop_prob = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model, optimizer and loss function\n",
    "model = Seq2Seq(input_size, hidden_size, hidden_size, output_size, drop_prob)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.words['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to tell us how long an epoch takes\n",
    "# taken from https://www.kaggle.com/code/columbine/seq2seq-pytorch\n",
    "def epoch_time(start_time, end_time):\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time  / 60)\n",
    "    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n",
    "    return  elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:34<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Time 3m 34s\"\n",
      "Train Loss: 9.410 | Train PPL: 12209.620\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:34<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Time 3m 34s\"\n",
      "Train Loss: 8.251 | Train PPL: 3831.780\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:36<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Time 3m 36s\"\n",
      "Train Loss: 7.819 | Train PPL: 2486.392\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:37<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Time 3m 37s\"\n",
      "Train Loss: 7.403 | Train PPL: 1641.561\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:36<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Time 3m 36s\"\n",
      "Train Loss: 6.909 | Train PPL: 1000.966\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:34<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Time 3m 34s\"\n",
      "Train Loss: 6.361 | Train PPL: 578.958\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:36<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Time 3m 36s\"\n",
      "Train Loss: 5.930 | Train PPL: 376.243\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:37<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Time 3m 37s\"\n",
      "Train Loss: 5.628 | Train PPL: 278.065\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:35<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Time 3m 35s\"\n",
      "Train Loss: 5.350 | Train PPL: 210.608\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:35<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time 3m 35s\"\n",
      "Train Loss: 5.165 | Train PPL: 175.026\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:36<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Time 3m 36s\"\n",
      "Train Loss: 5.070 | Train PPL: 159.214\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:36<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Time 3m 36s\"\n",
      "Train Loss: 4.958 | Train PPL: 142.364\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:36<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Time 3m 36s\"\n",
      "Train Loss: 4.864 | Train PPL: 129.518\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:36<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Time 3m 36s\"\n",
      "Train Loss: 4.863 | Train PPL: 129.363\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:31<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Time 3m 31s\"\n",
      "Train Loss: 4.816 | Train PPL: 123.470\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:33<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Time 3m 33s\"\n",
      "Train Loss: 4.791 | Train PPL: 120.450\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:36<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Time 3m 36s\"\n",
      "Train Loss: 4.782 | Train PPL: 119.327\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:35<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Time 3m 35s\"\n",
      "Train Loss: 4.760 | Train PPL: 116.740\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:35<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Time 3m 35s\"\n",
      "Train Loss: 4.719 | Train PPL: 112.006\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:35<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Time 3m 35s\"\n",
      "Train Loss: 4.691 | Train PPL: 108.976\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:39<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Time 3m 39s\"\n",
      "Train Loss: 4.679 | Train PPL: 107.696\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:38<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Time 3m 38s\"\n",
      "Train Loss: 4.656 | Train PPL: 105.230\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:38<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Time 3m 38s\"\n",
      "Train Loss: 4.618 | Train PPL: 101.310\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:32<00:00,  1.47it/s]\n",
      "  0%|          | 0/312 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Time 3m 32s\"\n",
      "Train Loss: 4.626 | Train PPL: 102.078\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:37<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Time 3m 37s\"\n",
      "Train Loss: 4.590 | Train PPL:  98.473\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [04:15<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Time 4m 15s\"\n",
      "Train Loss: 4.571 | Train PPL:  96.675\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [04:38<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Time 4m 38s\"\n",
      "Train Loss: 4.560 | Train PPL:  95.605\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [04:35<00:00,  1.13it/s]\n",
      "  0%|          | 0/312 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Time 4m 35s\"\n",
      "Train Loss: 4.577 | Train PPL:  97.230\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [04:35<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Time 4m 35s\"\n",
      "Train Loss: 4.552 | Train PPL:  94.858\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [04:36<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Time 4m 36s\"\n",
      "Train Loss: 4.561 | Train PPL:  95.650\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the minimum training loss\n",
    "min_train_loss = float('inf')\n",
    "\n",
    "# training\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device=device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} | Time {epoch_mins}m {epoch_secs}s\"')\n",
    "    print(f'Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\"')\n",
    "    \n",
    "    # save the model if the validation loss is at a minimum value\n",
    "    if train_loss < min_train_loss:\n",
    "        min_train_loss = train_loss\n",
    "        torch.save(model.state_dict(), 'chatbot_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model with best training loss for optimization\n",
    "model.load_state_dict(torch.load('chatbot_model.pt'))\n",
    "\n",
    "# adjust probability for droput layers\n",
    "drop_prob = 0.4\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [04:33<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Time 4m 33s\"\n",
      "Train Loss: 4.551 | Train PPL:  94.725\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [04:32<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Time 4m 32s\"\n",
      "Train Loss: 4.538 | Train PPL:  93.508\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [04:32<00:00,  1.15it/s]\n",
      "  0%|          | 0/312 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Time 4m 32s\"\n",
      "Train Loss: 4.547 | Train PPL:  94.317\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [04:32<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Time 4m 32s\"\n",
      "Train Loss: 4.515 | Train PPL:  91.356\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [04:34<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Time 4m 34s\"\n",
      "Train Loss: 4.450 | Train PPL:  85.646\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [04:37<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Time 4m 37s\"\n",
      "Train Loss: 4.400 | Train PPL:  81.453\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [04:35<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | Time 4m 35s\"\n",
      "Train Loss: 4.366 | Train PPL:  78.689\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [04:35<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 | Time 4m 35s\"\n",
      "Train Loss: 4.321 | Train PPL:  75.287\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [04:34<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Time 4m 34s\"\n",
      "Train Loss: 4.284 | Train PPL:  72.509\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:42<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Time 3m 42s\"\n",
      "Train Loss: 4.260 | Train PPL:  70.795\"\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device=device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+31} | Time {epoch_mins}m {epoch_secs}s\"')\n",
    "    print(f'Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\"')\n",
    "    \n",
    "        # save the model if the validation loss is at a minimum value\n",
    "    if train_loss < min_train_loss:\n",
    "        min_train_loss = train_loss\n",
    "        torch.save(model.state_dict(), 'chatbot_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, device=device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg in tqdm(data_loader):\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output = model(src, trg, teacher_forcing_ratio=0.0)\n",
    "\n",
    "            # reshape output and target to calculate loss\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330/330 [02:58<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 12.697 | Test PPL: 326806.326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_model = Seq2Seq(input_size, hidden_size, hidden_size, output_size, drop_prob).to(device)\n",
    "test_model.load_state_dict(torch.load('chatbot_model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_loader, criterion)\n",
    " \n",
    "print(f\"Test Loss : {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high test loss is a sign of significant overfitting. Training with the whole dataset and a larger vocabulary would likely have led to a better result, but due to computational restraints and limited GPU ressources, I decided to clip the dataset to 10,000 samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(24112, 512)\n",
       "    (lstm): LSTM(512, 512, batch_first=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(24112, 512)\n",
       "    (lstm): LSTM(512, 512, batch_first=True)\n",
       "    (output): Linear(in_features=512, out_features=24112, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_model = Seq2Seq(input_size, hidden_size, hidden_size, output_size, drop_prob).to(device)\n",
    "chatbot_model.load_state_dict(torch.load('chatbot_model.pt'))\n",
    "chatbot_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(model=chatbot_model, vocab=vocab):\n",
    "    print(\"Hi, I'm, your ChatBot. Type 'quit' to exit the chat.\\n\")\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'quit' or input_sentence == 'q': break\n",
    "\n",
    "            # normalize sentence\n",
    "            tokenized_sentence = vocab.indexSentences(input_sentence)\n",
    "\n",
    "            # pad sentence\n",
    "            num_padding = max_len - len(tokenized_sentence)\n",
    "            padded_sentence = tokenized_sentence + [vocab.words['<pad>']] * num_padding\n",
    "                       \n",
    "            # create input tensor\n",
    "            input_tensor = torch.LongTensor(padded_sentence).to(device)\n",
    "            \n",
    "            # fill up batch with empty tensors to match model dimensions\n",
    "            tensor_list = [input_tensor] + [torch.zeros_like(input_tensor) for i in range(batch_size-1)]\n",
    "            input_batch = torch.stack(tensor_list)\n",
    "                     \n",
    "            # create empty target tensor\n",
    "            empty_trg = torch.zeros_like(input_batch).to(device)\n",
    "            \n",
    "            # create model ouput\n",
    "            output = model(src=input_batch, trg=empty_trg, teacher_forcing_ratio=0.0)\n",
    "            output = output[0,:,:]\n",
    "            _, answer_indices = torch.max(output, dim=1)\n",
    "            \n",
    "            # print('answer_indices:', answer_indices)\n",
    "            \n",
    "            # turn indices back into words\n",
    "            answer = vocab.IndexToWord(index.item() for index in answer_indices if index != 0)\n",
    "            print('ChatBot:', ' '.join([word for word in answer]))\n",
    "            print(' ')\n",
    "            \n",
    "        except KeyError:\n",
    "             print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm, your ChatBot. Type 'quit' to exit the chat.\n",
      "\n",
      "> Who was the first president of the United States of America?\n",
      "ChatBot: 000 people in Afghanistan be Whitehead to both geographically and culturally of the northeastern Indian subcontinent more important than a Tajik prison in August 24 about whether Turkic or Iranian peoples were the original inhabitants of Central Asia it can be overcome\n",
      " \n",
      "> What is the capital of France?\n",
      "ChatBot: 3 850 000 people in developing countries and artificial photosynthesis of the science forbids me to enter the world and the world and to the world around it can provide humanitarian assistance and security of the world as a Tajik prison in\n",
      " \n",
      "> When did the American Civil War end?\n",
      "ChatBot: 000 people and to 147 F minor Op 21 years BCE and to think of people and objects as remaining fundamentally the same things harmful by an aim to all entities with the world around it by a self accept his father\n",
      " \n",
      "> quit\n"
     ]
    }
   ],
   "source": [
    "chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes and Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPL is short for \"perplexity\". According to https://www.educative.io/answers/what-is-perplexity-in-nlp,\n",
    "\n",
    "*Perplexity is a standard that evaluates how well a probability model can predict a sample. When applied to language models like GPT, it represents the exponentiated average negative log-likelihood of a sequence. In essence, a lower perplexity score suggests that the model has a higher certainty in its predictions.*\n",
    "\n",
    "See https://towardsdatascience.com/perplexity-in-language-models-87a196019a94 for further information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful tutorial:\n",
    "https://www.kaggle.com/code/columbine/seq2seq-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatbot inspired by https://pytorch.org/tutorials/beginner/chatbot_tutorial.html and \n",
    "https://github.com/aockel/seq2seq-squad2."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
