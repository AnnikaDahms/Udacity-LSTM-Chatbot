{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchdata --upgrade typing-extensions torchtext torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torch\n",
    "import time\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import LongTensor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 78840 questions and 78840 answers in the training dataset.\n",
      "There are 8759 questions and 8759 answers in the validation dataset.\n",
      "There are 10570 questions and 10570 answers in the test dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8759    What important neopragmatist was Harthorne's s...\n",
       "8760    How was Whitehead's theory of gravitation rece...\n",
       "8761    What physicists in the field of quantum theory...\n",
       "8762    What affect  did the discovery of gravitationa...\n",
       "8763                        What are gravitational waves?\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and explore SQuAD1 data\n",
    "\n",
    "from torchtext.datasets import SQuAD1\n",
    "\n",
    "train, test = SQuAD1()   \n",
    "\n",
    "def LoadSQuAD(data):\n",
    "    df = {\"question\": [], \"answer\": []}\n",
    "    index = 0\n",
    "    for context, question, answers, indices in data:\n",
    "        if answers[0]:\n",
    "            df[\"question\"].append(question)\n",
    "            df[\"answer\"].append(answers[0])\n",
    "        index += 1\n",
    "    df_complete = pd.DataFrame.from_dict(df)\n",
    "    SRC = df_complete[\"question\"]\n",
    "    TRG = df_complete[\"answer\"]\n",
    "    return df_complete, SRC, TRG\n",
    "    \n",
    "SRC_and_TRG_train_complete, SRC_train_complete, TRG_train_complete = LoadSQuAD(train)\n",
    "len_val_data = SRC_train_complete.shape[0]//10\n",
    "\n",
    "SRC_train = SRC_train_complete.iloc[len_val_data:]\n",
    "SRC_val = SRC_train_complete.iloc[:len_val_data]\n",
    "TRG_train = TRG_train_complete.iloc[len_val_data:]\n",
    "TRG_val = TRG_train_complete.iloc[:len_val_data]\n",
    "\n",
    "_, SRC_test, TRG_test = LoadSQuAD(test)\n",
    "\n",
    "print('There are {} questions and {} answers in the training dataset.'.format(SRC_train.shape[0], TRG_train.shape[0]))\n",
    "print('There are {} questions and {} answers in the validation dataset.'.format(SRC_val.shape[0], TRG_val.shape[0]))\n",
    "print('There are {} questions and {} answers in the test dataset.'.format(SRC_test.shape[0], TRG_test.shape[0]))\n",
    "SRC_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a vocabulary class\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.index = {}\n",
    "        self.count = 0\n",
    "        self.words = {}\n",
    "    \n",
    "    # tokenize each sentence\n",
    "    def prepareText(self, text):\n",
    "        tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        return tokens\n",
    "    \n",
    "    # create a list of all words contained in the text\n",
    "    def indexWord(self, word):\n",
    "        if word not in self.words:\n",
    "            self.words[word] = self.count\n",
    "            self.index[str(self.count)] = word\n",
    "            self.count += 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # take in a sentence and returns a list of integers \n",
    "    def indexSentences(self, sentence):\n",
    "        tokens = self.prepareText(sentence)\n",
    "        return [self.words[token] for token in tokens]\n",
    "    \n",
    "    # fill a vocabulary object with contents\n",
    "    def fillVocab(self, series, print_every=1000):\n",
    "        self.indexWord('<pad>')\n",
    "        \n",
    "        count = 0\n",
    "        for sentence in series:\n",
    "            text = self.prepareText(sentence)\n",
    "            for t in text:\n",
    "                if(self.indexWord(t)):\n",
    "                    if count % print_every == 0:\n",
    "                        print('Adding word {} to our vocabulary.'.format(count))\n",
    "                    count += 1\n",
    "        print('Added {} words to vocabulary.'.format(len(self.words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding word 0 to our vocabulary.\n",
      "Adding word 10000 to our vocabulary.\n",
      "Adding word 20000 to our vocabulary.\n",
      "Adding word 30000 to our vocabulary.\n",
      "Adding word 40000 to our vocabulary.\n",
      "Adding word 50000 to our vocabulary.\n",
      "Adding word 60000 to our vocabulary.\n",
      "Added 64259 words to vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# instantiate a vocabulary object and fill it \n",
    "vocab = Vocab(name='SQuAD1_vocab')\n",
    "SRC_and_TRG_complete = pd.concat([SRC_train, TRG_train, SRC_val, TRG_val, SRC_test, TRG_test])\n",
    "vocab.fillVocab(SRC_and_TRG_complete, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('<pad>', 0), ('What', 1), ('important', 2), ('neopragmatist', 3), ('was', 4), ('Harthorne', 5), ('s', 6), ('student', 7), ('How', 8), ('Whitehead', 9), ('theory', 10), ('of', 11), ('gravitation', 12), ('received', 13), ('physicists', 14), ('in', 15), ('the', 16), ('field', 17), ('quantum', 18), ('have', 19), ('been', 20), ('influenced', 21), ('by', 22), ('affect', 23), ('did', 24), ('discovery', 25), ('gravitational', 26), ('waves', 27), ('on', 28), ('are', 29)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out first 30 items of the vocabulary\n",
    "dict(list(vocab.words.items())[:30]).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index and pad sentences to length of the longest sentence in the data set\n",
    "SRC_train_indices = [vocab.indexSentences(s) for s in SRC_train]\n",
    "TRG_train_indices = [vocab.indexSentences(s) for s in TRG_train]\n",
    "SRC_val_indices = [vocab.indexSentences(s) for s in SRC_val]\n",
    "TRG_val_indices = [vocab.indexSentences(s) for s in TRG_val]\n",
    "SRC_test_indices = [vocab.indexSentences(s) for s in SRC_test]\n",
    "TRG_test_indices = [vocab.indexSentences(s) for s in TRG_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences to max_length\n",
    "def padSequences(sequences, max_len):\n",
    "    padded_sequences = []\n",
    "    for s in sequences:\n",
    "        \n",
    "        # calculate the number of padding tokens needed\n",
    "        num_padding = max_len - len(s)\n",
    "        \n",
    "        # create a new sequence with padding tokens added to the end\n",
    "        padded_sequence = s + [vocab.words['<pad>']] * num_padding\n",
    "        \n",
    "        # convert the sequence to a LongTensor and add it to the list\n",
    "        padded_sequences.append(LongTensor(padded_sequence))\n",
    "    return padded_sequences\n",
    "\n",
    "# determine the maximum length of sentences\n",
    "max_len = max(max(len(s) for s in SRC_train_indices), \n",
    "              max(len(s) for s in TRG_train_indices), \n",
    "              max(len(s) for s in SRC_val_indices), \n",
    "              max(len(s) for s in TRG_val_indices),\n",
    "              max(len(s) for s in SRC_test_indices),\n",
    "              max(len(s) for s in TRG_test_indices))\n",
    "\n",
    "SRC_train_pad = torch.stack(padSequences(SRC_train_indices, max_len))\n",
    "TRG_train_pad = torch.stack(padSequences(TRG_train_indices, max_len))\n",
    "SRC_val_pad = torch.stack(padSequences(SRC_val_indices, max_len))\n",
    "TRG_val_pad = torch.stack(padSequences(TRG_val_indices, max_len))\n",
    "SRC_test_pad = torch.stack(padSequences(SRC_test_indices, max_len))\n",
    "TRG_test_pad = torch.stack(padSequences(TRG_test_indices, max_len))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "batch_size = 128  \n",
    "\n",
    "train_data = TensorDataset(SRC_train_pad, TRG_train_pad)\n",
    "val_data = TensorDataset(SRC_val_pad, TRG_val_pad)\n",
    "test_data = TensorDataset(SRC_test_pad, TRG_test_pad)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, drop_last=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm cell needs arguments input, (hidden state, cell state)'\n",
    "where for batched data input is (sequence lengt, batch size, input size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Encoder, Decoder and Seq2Seq modules\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_size, drop_prob=0.5):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # nn.Embedding provides a vector representation of the input\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        \n",
    "        # nn.LSTM expects the arguments [input, (hidden state, cell state)]\n",
    "        # for batched data input is expected to be (sequence lengt, batch size, input size)\n",
    "        # batch_first=True changes the order to (batch size, sequence length, input size)\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self, i):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the src vector\n",
    "        Outputs: o, the encoder outputs\n",
    "                h, the hidden state\n",
    "                c, the cell state\n",
    "        '''\n",
    "        embedded = self.embedding(i)\n",
    "        embedded = self.dropout(embedded)\n",
    "        o, (h, c) = self.lstm(embedded)\n",
    "        \n",
    "        return h, c\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, output_size, embedding_size, hidden_size):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size        \n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, i, h, c):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the target vector\n",
    "        Outputs: o, the prediction\n",
    "                h, the hidden state\n",
    "        '''       \n",
    "        i = i.unsqueeze(1)\n",
    "        embedded = self.embedding(i)\n",
    "        o, (h, c) = self.lstm(embedded, (h, c))\n",
    "        o = self.output(o.squeeze(0))\n",
    "        \n",
    "        return o, h, c       \n",
    "                \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_size, output_size, device=device):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size, embedding_size, drop_prob=drop_prob)\n",
    "        self.decoder = Decoder(output_size, embedding_size, hidden_size)\n",
    "        \n",
    "        assert self.encoder.hidden_size == self.decoder.hidden_size, \\\n",
    "            'hidden dimensions of encoder and decoder must be equal.'\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):      \n",
    "        \n",
    "        # create empty output tensor with shape (length of trg, batch size, trg vocab size)\n",
    "        # that will later be filled with the predictions of the decoder\n",
    "        outputs = torch.zeros(trg.shape[0], trg.shape[1], self.decoder.output_size).to(device)\n",
    "\n",
    "        # use last hidden state of encoder as initial state for decoder\n",
    "        decoder_hidden, decoder_cell = self.encoder(src)\n",
    "        \n",
    "        decoder_input = trg[:, 0]\n",
    "      \n",
    "        # loop through elements in batch\n",
    "        for t in range(1, trg.shape[1]):\n",
    "            decoder_output, decoder_hidden, decoder_cell = self.decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "            outputs[:, t, :] = decoder_output.view(*outputs[:, t, :].shape)\n",
    "            teacher_force = torch.rand(1) < teacher_forcing_ratio\n",
    "            # use token with highest score as output\n",
    "            top1 = decoder_output.argmax(2)\n",
    "            decoder_input = trg[:, t] if teacher_force else top1.squeeze(1)\n",
    "                 \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, train_loader, criterion, optimizer, device=device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for src, trg in tqdm(train_loader):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        # reshape output and target to calculate loss\n",
    "        # (slice off the first column and flatten output to 2 dim)\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device=device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg in val_loader:\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output = model(src, trg, teacher_forcing_ratio=0.0)\n",
    "\n",
    "            # reshape output and target to calculate loss\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "input_size = len(vocab.words)\n",
    "output_size = len(vocab.words)\n",
    "embedding_size = 256\n",
    "hidden_size = 256\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "drop_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model, optimizer and loss function\n",
    "model = Seq2Seq(input_size, hidden_size, hidden_size, output_size)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.words['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to tell us how long an epoch takes\n",
    "# taken from https://www.kaggle.com/code/columbine/seq2seq-pytorch\n",
    "def epoch_time(start_time, end_time):\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time  / 60)\n",
    "    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n",
    "    return  elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/615 [00:02<?, ?it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.31 GiB (GPU 0; 14.76 GiB total capacity; 13.27 GiB already allocated; 113.75 MiB free; 13.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-175980cc0941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-cb58128a39d1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.31 GiB (GPU 0; 14.76 GiB total capacity; 13.27 GiB already allocated; 113.75 MiB free; 13.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# initialize the minimum validation loss\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "# training\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device=device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device=device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} | Time {epoch_mins}m {epoch_secs}s\"')\n",
    "    print(f'Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\"')\n",
    "    print(f'Val Loss: {val_loss:.3f} | Val PPL: {math.exp(val_loss):7.3f}\"')\n",
    "    \n",
    "    # save the model if the validation loss is at a minimum value\n",
    "    if valid_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPL stands for \"perplexity\". According to https://www.educative.io/answers/what-is-perplexity-in-nlp,\n",
    "\n",
    "*Perplexity is a standard that evaluates how well a probability model can predict a sample. When applied to language models like GPT, it represents the exponentiated average negative log-likelihood of a sequence. In essence, a lower perplexity score suggests that the model has a higher certainty in its predictions.*\n",
    "\n",
    "See https://towardsdatascience.com/perplexity-in-language-models-87a196019a94 for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "best_model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_loader, criterion)\n",
    " \n",
    "print(f\"Test Loss : {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}\")\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful tutorial:\n",
    "https://www.kaggle.com/code/columbine/seq2seq-pytorch"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
